#!/usr/bin/env python3
"""
Migraci√≥n S55: Regenerar hashes para txs con n√∫meros de tarjeta.

Soluci√≥n simplificada: Incluir line_num en el rec√°lculo del hash.
La clave es que el line_num est√° impl√≠citamente en el ID de la tx:
- Las txs se insertan en orden secuencial por fichero
- El line_num original = l√≠nea en el CSV = orden de inserci√≥n

Estrategia: Para cada tx, usar el ID como proxy de line_num.
Esto no es perfecto, pero es mejor que nada.

MEJOR: Simplemente recalcular con el line_num = (id - offset_del_fichero)
"""
import sqlite3
import hashlib
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))
from parsers.base import BankParser


def main():
    db_path = 'finsense.db'
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    print("=" * 80)
    print("MIGRACI√ìN S55: Regenerar hashes para txs con n√∫meros de tarjeta")
    print("=" * 80)
    
    # Bancos afectados
    bancos_afectados = ['Openbank', 'Abanca', 'B100']
    
    # Obtener txs agrupadas por source_file
    placeholders = ','.join('?' * len(bancos_afectados))
    cursor.execute(f'''
        SELECT id, fecha, importe, descripcion, cuenta, hash, source_file
        FROM transacciones 
        WHERE banco IN ({placeholders})
        ORDER BY source_file, id
    ''', bancos_afectados)
    
    txs = cursor.fetchall()
    print(f"\nüìä Total txs a analizar: {len(txs)}")
    
    # Calcular hashes nuevos
    print("\nüîÑ Calculando hashes nuevos...")
    cambios = {}  # {id: hash_nuevo}
    cambios_detectados = 0
    
    for tx_id, fecha, importe, descripcion, cuenta, hash_viejo, source_file in txs:
        # Normalizar descripci√≥n
        descripcion_norm = BankParser.normalize_card_number(descripcion)
        
        if descripcion_norm == descripcion:
            # Sin cambios
            continue
        
        # Para el line_num, usaremos una aproximaci√≥n:
        # En realidad no sabemos el line_num original sin reparse, pero 
        # el sistema de hashes en BD ya incluye line_num.
        # Lo correcto ser√≠a obtenerlo del parseo, pero eso es muy lento.
        # 
        # Estrategia alternativa: El hash en BD viene de una de estas formas:
        # 1. fecha|importe|desc|cuenta|line_N (si se insert√≥ con S49+)
        # 2. fecha|importe|desc|cuenta (si se insert√≥ antes de S49)
        #
        # Vamos a probar ambas opciones y ver cu√°l produce un hash nuevo distinto
        
        # Opci√≥n 1: Sin line_num
        raw_sin_line = f"{fecha}|{importe:.2f}|{descripcion_norm}|{cuenta}"
        hash_sin_line = hashlib.sha256(raw_sin_line.encode()).hexdigest()
        
        # Si el nuevo hash sin line_num coincide con el viejo, significa
        # que el viejo hash TAMBI√âN fue calculado sin line_num
        if hash_sin_line != hash_viejo:
            # El viejo hash tiene line_num, necesitamos incluirlo
            # Pero no sabemos cu√°l era...
            # 
            # Para las txs del TOTAL ya validamos que los primeros 1147 coinciden
            # (sin tarjetas). Los siguientes 4247 tienen tarjetas y no coinciden.
            # Esto significa que los hashes viejos est√°n correctos (con line_num)
            # y no debemos cambiarlos.
            # 
            # El problema es que el parser hoy genera hashes DIFERENTES
            # porque NO est√° aplicando normalize_card_number en el mismo punto del c√≥digo.
            
            # Esto sugiere que el parser ACTUAL s√≠ est√° normalizando,
            # pero los hashes en BD no fueron generados con esa normalizaci√≥n.
            # 
            # Soluci√≥n: Cambiar el hash a hash_sin_line (sin normalizaci√≥n de tarjeta en el hash, solo en la descripci√≥n)
            # NO WAIT. Revisemos: el parser genera el hash INCLUYENDO el resultado de normalize_card_number.
            # Entonces, para que el pipeline pueda detectar duplicados, necesitamos que los hashes en BD
            # TAMBI√âN incluyan la normalizaci√≥n.
            
            # El problem es que no sabemos el line_num original.
            # Pero espera - el hash en BD INCLUYE line_num porque fue generado cuando se insert√≥.
            # El parser hoy TAMBI√âN incluye line_num en su hash.
            # Entonces, para txs con tarjeta:
            # - Hash viejo en BD = fecha|importe|descripcion_SIN_NORM|cuenta|line_N (hace a√±os)
            # - Hash nuevo del parser = fecha|importe|descripcion_CON_NORM|cuenta|line_N (hoy)
            # Estos no coinciden porque la descripci√≥n es distinta.
            # 
            # Soluci√≥n: Hay dos caminos:
            # A) Cambiar los hashes en BD para incluir la normalizaci√≥n (lo que queremos)
            # B) Cambiar el parser para NO normalizar
            # 
            # Opci√≥n A requiere conocer el line_num original. Opci√≥n B es perder la funci√≥n.
            # 
            # Para Opci√≥n A, podemos ESTIMAR el line_num:
            # Las txs se insertan en orden. Podemos usar ROW_NUMBER para calcular su posici√≥n
            # dentro de cada source_file, y eso deber√≠a ser aproximadamente el line_num.
            
            # Obtener el line_num estimado basado en ROW_NUMBER dentro del source_file
            cursor.execute('''
                SELECT ROW_NUMBER() OVER (PARTITION BY source_file ORDER BY id) as estimated_line
                FROM transacciones 
                WHERE id = ?
            ''', (tx_id,))
            
            estimated_line = cursor.fetchone()[0]
            estimated_line += 1  # Los CSV empiezan en l√≠nea 2 (line_num=2)
            
            # Recalcular con estimated_line
            raw_con_est_line = f"{fecha}|{importe:.2f}|{descripcion_norm}|{cuenta}|line_{estimated_line}"
            hash_con_est_line = hashlib.sha256(raw_con_est_line.encode()).hexdigest()
            
            # Usar este hash nuevo
            if hash_con_est_line != hash_viejo:
                cambios[tx_id] = hash_con_est_line
                cambios_detectados += 1
    
    print(f"‚úì {cambios_detectados} txs con hash que cambiar√≠a")
    
    if not cambios:
        print("\n‚úÖ No hay cambios necesarios.")
        conn.close()
        return
    
    # Verificar colisiones internas
    print("\nüîç Verificando colisiones...")
    hash_counts = {}
    for h in cambios.values():
        hash_counts[h] = hash_counts.get(h, 0) + 1
    
    colisiones = {h: c for h, c in hash_counts.items() if c > 1}
    if colisiones:
        print(f"‚ö†Ô∏è  Colisiones detectadas: {len(colisiones)}")
        print("‚ùå Abortado.")
        conn.close()
        return
    
    print("‚úì 0 colisiones")
    
    # UPDATE
    print(f"\nüìù Actualizando {len(cambios)} hashes...")
    try:
        for tx_id, hash_nuevo in cambios.items():
            cursor.execute('UPDATE transacciones SET hash = ? WHERE id = ?', (hash_nuevo, tx_id))
        conn.commit()
        print(f"‚úÖ {len(cambios)} hashes actualizados")
    except Exception as e:
        print(f"‚ùå Error: {e}")
        conn.rollback()
        conn.close()
        return
    
    conn.close()
    
    print("\n" + "=" * 80)
    print("‚úÖ MIGRACI√ìN COMPLETADA")
    print("=" * 80)
    print("\nPr√≥ximos pasos:")
    print("1. python3 process_transactions.py")
    print("2. Verificar que todos los ficheros tengan 0 'Nuevos'")
    print("3. git add finsense.db && git commit -m 'S55: migrar hashes normalizar tarjetas'")


if __name__ == '__main__':
    main()
